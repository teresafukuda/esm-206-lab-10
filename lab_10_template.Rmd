---
title: "ESM 206 Lab 10"
author: "Teresa Fukuda"
date: "December 3, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Lab 10 Objectives:

- Correlation matrices
- Binary logistic regression
- Dates/times with lubridate
- Heatmap of solar irradiation in Santa Barbara, Hilo (HI), and Fairbanks (AK) in 2010
- Aquaculture with gganimate

# PART 0. Load libraries
```{r packages}

library(tidyverse) # The tidyverse!
library(corrplot) # For correlation matrices
library(janitor) # For cleaning up column names
library(lubridate) # For dealing with dates & times
library(gganimate) # For adding animation to ggplot graphics
library(ggimage) # For updating graph components with images


```

# PART 1. Correlation matrix (World Bank world environmental factors data)

Compiled World Bank data, accessed from: https://www.kaggle.com/zanderventer/environmental-variables-for-world-countries#World_countries_env_vars.csv

- Read in the "world_env_vars.csv file" as env_var
- Find correlations between all variables, columns 2:28
- Plot the correlation matrix (corrplot)

```{r env_corr}

env_var <- read_csv("world_env_vars.csv") %>% 
  na.omit # compete case omission-- remove anything with NA
# not always recommended to remove datat this way
# many environmental variables for different places-- how do they correlate? 

# Calculate Pearson's r for correlations
# does not makes sense to include the country name column, so want to use col 2-28

# new correlation df
# cor does not want NA values
cor_df <- cor(env_var[2:28]) # in brackets specifies columns 2-28

# visually look at this using corrplot function to make correlation matrix
corrplot(cor_df, #input matrix of correlation data, not original data
         type= "upper",# only shows upper half of matrix (so as not to repeat data)
         method= "ellipse",
         tl.col="black", # change label text color and size
         tl.cex=0.5) 

# use for continuous predictor variables

# method ellipse direction indicates directionality of correlation 

```

# PART 2. Binary Logistic Regression (Donner Party Data)

Use the 'glm' function for fitting *generalized linear models* (the *logit* - log odds of survival, in our case, will be linearly related to Sex and Age. So we expect the final model to look something like this: 

$$Log Odds (Survival) = \beta_0 + \beta_1(Age) + \beta_2(Sex)$$

We'll use 'family = binomial' to run binomial logistic regression...otherwise, this looks very similar to other types of regression we've already done. 

a. Read in the DonnerTable.csv file as DonnerTable

```{r donner_data}

# what effect do age and sex have on the probability of survival in the Donner party? 

DonnerTable <- read_csv("DonnerTable.csv")

# A TIP: when the outcome dependent variable is ALREADY coded as 0 and 1, that is really nice because there is no question about whether you are trying to find the p of outcome A or B. IF you have a binary dependent variable that is T/F, important to find out how R codes that. Easier to specify yourself in a new column (use casewhen) that Y=0 and N=1, for example.

# use the GLM funtion (generalized linear model), where the output is the logit (log odds)

```

b. Binomial logistic regression

```{r blr}

# family specifies type of model
donner_blr <- glm(Outcome ~ Sex + Age, family= "binomial", data=DonnerTable)

summary(donner_blr)

# output is giving log odds
# both coefficients for sexmale and age are negative, showing that based on the table, male survival is lower and increase in age led to a decrease in likelihood of survival 

# what does this mean in terms of actual probabilities? 

```

c. Use the model to answer: What are the log odds of survival for a 5 year-old female? The probability of survival?

```{r predict_f_5}

# 1. Create a data frame with variables Sex and Age, containing data "Female" and 5, respectively: 

f_5 <- data.frame(Sex="Female", Age=5) 

## variables in new dataframe must be the same as the original inputs of the model, in this case must be "Sex" and "Age". 


# 2. Find the log odds of survival for the new data (5 year old female) using predict() function with type = "link":

f_5_logodds <- predict(donner_blr, newdata= f_5, type= "link")
## type will include a prediction for the actual meteric of the dependent variable, meaning that it will predict the log odds.


# 3. Exponentiate the log odds to find ODDS of survival for a 5 year old female:

f_5_odds <-exp(f_5_logodds)
## odds of survival are 4:1 -- probability is about 80% = 4/5


# Ask: Could we manually find the probability of survival for a 5-year old female? recall: p/(1-p) = ODDS

## p= 4/5=80%

# 4. Actually, let's just use type = "response" in the predict function, which converts to a probability for us:

f_5_prob <- predict(donner_blr, newdata=f_5, type= "response")
# probability is much easier to undestand than log odds!!!!! to anyone other than a scientific/statistical audience, always use probability


```

d. What is the probability of survival for a 25 year-old male? 

```{r predict_m_25}

# Similarly:

m_25 <- data.frame(Sex = "Male", Age = 25) # Make a new data frame

m_25_prob <- predict(donner_blr, newdata = m_25, type = "response") # Find probability of survival
m_25_prob

# odds of being a 25 year old male are way worse for an older male than a 5 year old female; this makes sense because the women and children were staying put, while the men were going out to try to find help and they were dying. This is interesting. What if we want to find out what happens to the probability of survival over a range of ages and for both sexes?

```

e. Create new sequences of data so that we can graph probabilities for the entire spectrum of ages, designated by sex.

```{r new_data}

seq_age <- rep(seq(from = 0, to = 100), 2) # Create a vector with sequence from 0 to 100, twice (one will be "Male" and one will be "Female")

f_101 <- rep("Female", 101) # Repeat 'Female' 101 times (to match years data)
m_101 <- rep("Male", 101) # Repeat 'Male' 101 times
mf_101 <- c(f_101, m_101) # Combine them into a single vector

# Combine the age and sex sequences into a single data frame - that will be the new data that we have our model make predictions for

donner_newdata <- data.frame(seq_age, mf_101) # MUST make column names match variables in the model!
colnames(donner_newdata) <- c("Age","Sex")

```

f. Now that we have new data to put into our model to have it make predictions, let's go ahead and actually find the predicted probabilities for each Age/Sex combination.

```{r donner_predict}

# Find probabilities using predict (with type = "response"). Include SE.

predicted_probs <- predict(donner_blr, newdata=donner_newdata, type= "response", se.fit=TRUE) # se.fit=TRUE calculates a standard error

# Coerce outcome into data frame. 

graph_data <- data.frame (donner_newdata, predicted_probs$fit, predicted_probs$se.fit)

colnames(graph_data) <- c("Age","Sex","probability","SE")

```

g. Graph results.

```{r donner_graph}

ggplot(graph_data, aes(x= Age, y= probability)) +
  geom_line(aes(color=Sex)) + # tells R that there are different series 
  geom_ribbon(aes(ymin= probability-SE, ymax= probability +SE, fill=Sex), alpha = 0.4)
  
# in order to create a ribbon plot, we set the upper and lower bounds for the ribbon (in this case, the prob Â± SE)

# creates a graph where the lines show predicte probability of survival, and the ribbons show standard error

# geom_smooth is great for a linear trend where you are looking at a confidence interval, whereas you might use geom_ribbon for non linear information 
  
```


# PART 3. Solar irradiation at the 3 locations in 2010 (pull in all together, do some lubridate stuff, etc.)

a. Read in multiple solar irradiation files (for SB (CA), Hilo (HI), and Fairbanks (AK)):
```{r solar_data}

# we have data collected at different places, but they have the same format (same column names); we can read those all in the same time to one df; especially easy if they have something similar in the file prefix CONSISTENCY IS GREAT!!!!!!!


si_full <- list.files(pattern= "solar_irradiation_*") %>%  # match anything that starts with "solar_irratiation"
  map_df(~read_csv(.)) %>% # read in these together into a single dataframe
  clean_names()

# this is useful because there was a separate column in each file that showed the site name (so now we can still tell which one is which)

# these column names are awful. (1) you could fix these in excel (2) use rename in R (3) use janitor package to help you-- it makes the names snakecase!! use function clean_names()

#### right now R does not see the dates and times as actual dates and times (just characters)

```

b. Wrangle the data
```{r wrangle_solar}

# tidy this up!

solar_tidy <- si_full %>% 
  rename(sol_rad =etr_wh_m_2,
         date= yyyy_mm_dd,
         time= hh_mm_lst) %>% 
  filter(time!="NA") %>% 
  mutate(site=fct_relevel(site, "Hawaii","Santa Barbara","Alaska"))
  

```

c. Use lubridate() functions to convert to times/dates
```{r date_time}

solar_tidy$date <- mdy(solar_tidy$date) #reassign the data in date column as month day year

solar_tidy$time <-hms(solar_tidy$time)

```

d. Make an awesome figure of solar irradiation (heat/tile map)
```{r solar_graph}

solar_gg <- ggplot(solar_tidy, aes(x=date, y= time)) +
  geom_tile(aes(fill=sol_rad)) +
  scale_fill_gradientn(colors=c("royalblue2","mediumorchid1","orange","yellow")) +
  scale_y_time() +
  facet_wrap(~site, ncol =1)

solar_gg

#heat maps can be useful when you have values for a variable that are affected by another variable (i.e. sunlight over date and time)

```


#PART 4. gganimate example: total aquaculture production (metric tons) for United States, Brazil, Chile, and Ecuador

a. Get the data, and do some wrangling:
```{r aqua_data}


# read in the aquaculture data from world something
aq_df <- read_csv("aq_wb.csv")

# data is in wide format: years are columns THIS IS NOT TIDY.

# wrangle this

aq_tidy <- aq_df %>% 
  filter(country == "Brazil" |
           country == "Chile" |
           country == "Ecuador" |
           country == "United States") %>% 
  gather(year, aq_production, `1960`:`2016`) %>% # gather will take all the column headers into the first column called year, and then will take all values from those columns into a second column called aq_production; must give it all of the columns that we are going to take and gather-- use backticks to show that it is the column names and not the values
  filter(year>= 1990) %>% 
  mutate(aq_mil = aq_production/1e6) %>% 
  select(country, year, aq_mil)


```


b. Read in the fish.png as 'fish'

```{r fish_image}

fish <- "fish.png"


```

c. Make a graph...with gganimate!
```{r gganimate_fish}

aq_plot <- ggplot(aq_tidy, aes(x=as.numeric(year),y=aq_mil, group=country)) +
  geom_line(aes(color=country)) +
  geom_image(aes(image=fish)) +
  geom_text(aes(label=country, color= country), position= position_nudge(y=0.04, x=1), size =5) +
  transition_reveal(country, as.numeric(year)) # will reveal by country, and then increments will be as.numeric(year)

aq_plot

animate (aq_plot, nframes=24, renderer=gifski_renderer("aq_animate.gif"))

```

##END LAB